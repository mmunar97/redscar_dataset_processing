{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b29363fe",
   "metadata": {},
   "source": [
    "# Infection detection in abdominal surgery images using fuzzy sets\n",
    "\n",
    "In this booklet, we set up all the experimentation for infection detection. The method, as well as a detailed explanation of the method, is taken from:\n",
    "\n",
    "- M. González-Hidalgo, M. Munar, P. Bibiloni, G. Moyà-Alcover, A. Craus-Miguel and J. J. Segura-Sampedro, \"Detection of infected wounds in abdominal surgery images using fuzzy logic and fuzzy sets,\" 2019 International Conference on Wireless and Mobile Computing, Networking and Communications (WiMob), Barcelona, Spain, 2019, pp. 99-106, doi: 10.1109/WiMOB.2019.8923289.\n",
    "- M. González-Hidalgo, G. Moyà-Alcover, M. Munar, P. Bibiloni, A. Craus-Miguel, X. González-Argenté and J.J Segura-Sampedro, \"Detection and Automatic Deletion of Staples in Images of Wound of Abdominal Surgery for m-Health Applications\". In: Tavares, J., Natal Jorge, R. (eds) VipIMAGE 2019. VipIMAGE 2019. Lecture Notes in Computational Vision and Biomechanics, vol 34. Springer, Cham. https://doi.org/10.1007/978-3-030-32040-9_23\n",
    "\n",
    "Note that these two methods receive an image containing only the wound. Since different neural networks have been trained, and it has been concluded that the Double UNet architecture has the best performance in wound detection, we will use this method as a wound mask generator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05640e9",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85819512",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import glob\n",
    "import json\n",
    "import numpy\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\" # Use only if there are available GPUs, and select which one has to be used.\n",
    "import skimage\n",
    "\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "#Custom library with the implementation of the considered CNN architectures.\n",
    "#Important: It must be used the version 1.0 of this library.\n",
    "from cnn_architectures.architectures.base.CNNModel import CNNModel\n",
    "from cnn_architectures.architectures.models.double_unet.double_unet import DoubleUNet\n",
    "from cnn_architectures.architectures.models.unet.unet import UNet\n",
    "\n",
    "#Custom library with the implementation of the staples detection methods.\n",
    "from staples_detection.base.staple_detection_methods import StapleDetectionMethod\n",
    "from staples_detection.staple_detector import StapleDetector\n",
    "\n",
    "#Custom library with the implementation of some well-known inpainting methods in the literature.\n",
    "from inPYinting.base.inpainting_algorithms import InpaintingAlgorithm\n",
    "from inPYinting.inpainter import Inpainter\n",
    "\n",
    "#Custom library with the implementation of the fuzzy sets-based colour segmentation algorithms.\n",
    "from colour_segmentation.base.segmentation_algorithm import SegmentationAlgorithm\n",
    "from colour_segmentation.segmentator import Segmentator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633d72f5",
   "metadata": {},
   "source": [
    "## Dataset load"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2720e061",
   "metadata": {},
   "source": [
    "First, we establish the main path of the dataset. Naturally, this will depend on the location of the dataset within the user's computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5622fa61",
   "metadata": {},
   "outputs": [],
   "source": [
    "REDSCAR_DATASET_PATH = r\"/home/marc/UIB_EXPERIMENTS/REDSCAR/SUBSETS/MACHINE_LEARNING_DATASET\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243eeead",
   "metadata": {},
   "source": [
    "Now, we set up the routes for the Redscar Machine Learning train and test sets. This should not be modified, as it is intrinsic to the dataset itself. All image names are stored in a list. There is one list for train and one for test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c56214",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATASET = os.path.join(REDSCAR_DATASET_PATH, \"train\")\n",
    "TEST_DATASET = os.path.join(REDSCAR_DATASET_PATH, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53b0df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(REDSCAR_DATASET_PATH, \"test.txt\")) as test_images_names_file:\n",
    "    test_images_names = [line.rstrip() for line in test_images_names_file]\n",
    "    \n",
    "with open(os.path.join(REDSCAR_DATASET_PATH, \"train.txt\")) as train_images_names_file:\n",
    "    train_images_names = [line.rstrip() for line in train_images_names_file]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ca3b5a",
   "metadata": {},
   "source": [
    "## Neural network-based models for staple and wound segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e370b34",
   "metadata": {},
   "source": [
    "We proceed to load the two models trained for staples and wound segmentation in images. The models to be considered are the following:\n",
    "- For staples segmentation, the model based on the UNet architecture trained with 100 epochs.\n",
    "- For wound segmentation, the model based on the Double UNet architecture trained with 100 epochs.\n",
    "\n",
    "In both cases, the models considered are the ones that have offered the best performance in their respective tasks. All CNN models used in the study are implemented in the `cnn_architectures` library, so we must continue to use this library for model loading."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b25ab4d",
   "metadata": {},
   "source": [
    "### UNet model for staples segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a175493",
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_model_path = \"/home/marc/UIB_EXPERIMENTS/CNN_EXPERIMENTS/UNET/EXPERIMENT4/unet3_epochs=100_lr=3e-4_res=0.h5\"\n",
    "unet_model = UNet(input_size=(512, 512, 3),\n",
    "                  out_channel=1, \n",
    "                  batch_normalization=True)\n",
    "unet_model.build(n_filters=64, dilation_rate=1, layer_depth = 5, last_activation=\"sigmoid\")\n",
    "unet_model.load_weight(unet_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a097b394",
   "metadata": {},
   "source": [
    "### Double UNet model for wound segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda5086b",
   "metadata": {},
   "outputs": [],
   "source": [
    "doubleunet_model_path = \"/home/marc/UIB_EXPERIMENTS/CNN_EXPERIMENTS/DoubleUNet/EXPERIMENT1/dun3_epochs=100_lr=3e-5_res=0.h5\"\n",
    "\n",
    "doubleunet_model = DoubleUNet(input_size=(256, 256, 3))\n",
    "doubleunet_model.build()\n",
    "doubleunet_model.load_weight(doubleunet_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe1ea01",
   "metadata": {},
   "source": [
    "Although the `CNNModel` object of the `cnn_architectures` library already has a predefined method for prediction, it predicts and returns the mask in the size of the network. To avoid doing size transformations on the binarised image, we propose the following method to predict and return the result in the same size. The `predict_binary` method of `CNNModel` has been slightly adapted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b0d6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prediction(image: numpy.ndarray, model: CNNModel, binary_threshold: float=0.5) -> Tuple[numpy.ndarray, numpy.ndarray, numpy.ndarray]:\n",
    "    # The input image is resized to fit the size of the network.\n",
    "    resized_image_normalized = skimage.transform.resize(image, (model.input_size[0], model.input_size[1], model.input_size[2]))\n",
    "    \n",
    "    # The prediction is made.\n",
    "    prediction = model.model.predict(numpy.array([resized_image_normalized]))\n",
    "    if isinstance(model, DoubleUNet):\n",
    "        prediction = prediction[0][:, :, 1] # Remember: The Double UNet presents two outputs. It is the second that is intended to be obtained.\n",
    "    else:\n",
    "        prediction = prediction[0][:, :, 0]\n",
    "        \n",
    "    # It is resized again to generate a mask the same as the input image.\n",
    "    final_prediction_raw = skimage.transform.resize(prediction, (image.shape[0], image.shape[1]))\n",
    "    final_prediction_binary = final_prediction_raw >= binary_threshold\n",
    "    final_prediction_int = (final_prediction_binary*255).astype(numpy.uint8)\n",
    "    \n",
    "    return final_prediction_raw, final_prediction_binary, final_prediction_int"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c74cf5",
   "metadata": {},
   "source": [
    "## Experimentation\n",
    "\n",
    "For each image we have, we will follow the method proposed in both reference works. Specifically, the steps to follow are:\n",
    "\n",
    "1. Predict the location of the staples and wound using the pre-trained UNet and Double UNet models, respectively. \n",
    "2. The wound located in step (1) may have different connected components. Then, for each connected component:\n",
    "    1. Obtain the coordinates of the bounding box that delimits it. This is only to be able to obtain the region of the wound to be studied, and will therefore be the input for the previous methods.\n",
    "    2. Remove the staples using the mask obtained in the first step. \n",
    "    3. With the staples removed, Liu and Shamir's chromatic segmentation methods, based on fuzzy set membership functions, are applied. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876f242e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def process_image(image: numpy.ndarray, image_name: str, cnn_model_staples: UNet, cnn_model_wound: DoubleUNet) -> Dict:\n",
    "    results = {}\n",
    "    results[\"IMAGE_NAME\"] = image_name\n",
    "    \n",
    "    # Computation of the mask with the position of the wound\n",
    "    _, _, prediction_mask_staples = generate_prediction(image=image, model=cnn_model_staples)\n",
    "    _, _, prediction_mask_wound = generate_prediction(image=image, model=cnn_model_wound)\n",
    "    \n",
    "    # Computation of the connected components of the mask\n",
    "    #number_connected_components, labels_connected_components = cv2.connectedComponents(prediction_mask_wound)\n",
    "    connected_component_output = cv2.connectedComponentsWithStats(prediction_mask_wound)\n",
    "    number_connected_components = connected_component_output[0]\n",
    "    labels_connected_components = connected_component_output[1]\n",
    "    areas_connected_components = connected_component_output[2][:,cv2.CC_STAT_AREA]\n",
    "    \n",
    "    # Uncomment the following line to add the predicted mask to the dictionary of results\n",
    "    # results[\"PREDICTED_WOUND_MASK\": prediction_mask_wound]\n",
    "    results[\"NUMBER_CONNECTED_COMPONENTS\"] = number_connected_components-1\n",
    "    \n",
    "    # According to the documentation of connectedComponents provided by OpenCV, the 0 index is always reserved to background\n",
    "    for connected_component_label in range(1, number_connected_components):\n",
    "        \n",
    "        # We select the corresponding component mask, and calculate the staple mask as the \n",
    "        # intersection of the component wound mask and the global staple mask.\n",
    "        wound_segment_mask = numpy.zeros((image.shape[0], image.shape[1]), dtype=numpy.uint8)\n",
    "        wound_segment_mask[labels_connected_components == connected_component_label] = 255\n",
    "        \n",
    "        staples_mask = prediction_mask_staples & wound_segment_mask\n",
    "        \n",
    "        # The coordinates of this connected component are obtained\n",
    "        contours = cv2.findContours(wound_segment_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        contours = contours[0][0]\n",
    "        \n",
    "        x,y,w,h = cv2.boundingRect(contours)\n",
    "        wound_region = image[y:y+h, x:x+w, :]\n",
    "        staples_mask = staples_mask[y:y+h, x:x+w]\n",
    "        \n",
    "        \n",
    "        inpainter = Inpainter(image=wound_region, mask=staples_mask.astype(numpy.uint8))\n",
    "        inpainting_result = inpainter.inpaint(InpaintingAlgorithm.NAVIER_STOKES)\n",
    "        \n",
    "        segmentator = Segmentator(image=inpainting_result.inpainted_image)\n",
    "        result_liu = segmentator.segment(method=SegmentationAlgorithm.FUZZY_SET_LIU,\n",
    "                                         remove_achromatic_colours=False)\n",
    "        result_shamir = segmentator.segment(method=SegmentationAlgorithm.FUZZY_SET_SHAMIR,\n",
    "                                            remove_achromatic_colours=False)\n",
    "        result_chamorro = segmentator.segment(method=SegmentationAlgorithm.FUZZY_SET_CHAMORRO,\n",
    "                                              remove_achromatic_colours=False)\n",
    "        result_amante = segmentator.segment(method=SegmentationAlgorithm.FUZZY_SET_AMANTE,\n",
    "                                            remove_achromatic_colours=False)\n",
    "        \n",
    "        result_liu_chr = segmentator.segment(method=SegmentationAlgorithm.FUZZY_SET_LIU,\n",
    "                                             remove_achromatic_colours=True)\n",
    "        result_shamir_chr = segmentator.segment(method=SegmentationAlgorithm.FUZZY_SET_SHAMIR,\n",
    "                                                remove_achromatic_colours=True)\n",
    "        result_chamorro_chr = segmentator.segment(method=SegmentationAlgorithm.FUZZY_SET_CHAMORRO,\n",
    "                                                  remove_achromatic_colours=True)\n",
    "        result_amante_chr = segmentator.segment(method=SegmentationAlgorithm.FUZZY_SET_AMANTE,\n",
    "                                                remove_achromatic_colours=True)\n",
    "        \n",
    "        # Uncomment the following lines to add the partial results to the dictionary of results\n",
    "        # results[rf\"WOUND_region={connected_component_label}\"] = wound_region\n",
    "        # results[rf\"STAPLES_region={connected_component_label}\"] = staples_mask\n",
    "        # results[rf\"INPAINTING_region={connected_component_label}\"] = inpainting_result.inpainted_image\n",
    "        # results[rf\"LIU_COLOR_SEGMENTATION_region={connected_component_label}\"] = result_liu.segmented_image\n",
    "        # results[rf\"SHAMIR_COLOR_SEGMENTATION_region={connected_component_label}\"] = result_shamir.segmented_image\n",
    "        \n",
    "        results[rf\"LIU_COLOR_REDPROP_region={connected_component_label}\"] = result_liu.get_colour_proportion(colour_label=0)\n",
    "        results[rf\"SHAMIR_COLOR_REDPROP_region={connected_component_label}\"] = result_shamir.get_colour_proportion(colour_label=0)\n",
    "        results[rf\"CHAMORRO_COLOR_REDPROP_region={connected_component_label}\"] = result_chamorro.get_colour_proportion(colour_label=0)\n",
    "        results[rf\"AMANTE_COLOR_REDPROP_region={connected_component_label}\"] = result_amante.get_colour_proportion(colour_label=0)\n",
    "        \n",
    "        results[rf\"LIU_COLOR_CHR_REDPROP_region={connected_component_label}\"] = result_liu_chr.get_colour_proportion(colour_label=0)\n",
    "        results[rf\"SHAMIR_COLOR_CHR_REDPROP_region={connected_component_label}\"] = result_shamir_chr.get_colour_proportion(colour_label=0)\n",
    "        results[rf\"CHAMORRO_COLOR_CHR_REDPROP_region={connected_component_label}\"] = result_chamorro_chr.get_colour_proportion(colour_label=0)\n",
    "        results[rf\"AMANTE_COLOR_CHR_REDPROP_region={connected_component_label}\"] = result_amante_chr.get_colour_proportion(colour_label=0)\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed011db2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3556763b",
   "metadata": {},
   "source": [
    "Finally, we run all the images. Attention: this cell has a high computational cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b280a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_train_images_results = []\n",
    "\n",
    "for image_name in tqdm(train_images_names):\n",
    "    image_path = os.path.join(TRAIN_DATASET, \"IMAGES\", image_name)\n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "    result = process_image(image=image, \n",
    "                           image_name=image_name, \n",
    "                           cnn_model_staples=unet_model, \n",
    "                           cnn_model_wound=doubleunet_model)\n",
    "    experiment_train_images_results.append(result)\n",
    "\n",
    "with open('train_redness_evaluation.json', 'w') as fout:\n",
    "    json.dump(experiment_train_images_results, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b36f7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_test_images_results = []\n",
    "\n",
    "for image_name in tqdm(test_images_names):\n",
    "    image_path = os.path.join(TEST_DATASET, \"IMAGES\", image_name)\n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "    result = process_image(image=image, \n",
    "                           image_name=image_name, \n",
    "                           cnn_model_staples=unet_model, \n",
    "                           cnn_model_wound=doubleunet_model)\n",
    "    experiment_test_images_results.append(result)\n",
    "\n",
    "with open('test_redness_evaluation.json', 'w') as fout:\n",
    "    json.dump(experiment_test_images_results, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00df7f83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "redscar_experiments",
   "language": "python",
   "name": "redscar_experiments"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
